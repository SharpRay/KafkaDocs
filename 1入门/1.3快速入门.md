# 快速入门

本教程假设你是新手并且没有已存在的Kafka或Zookeeper数据。

## 第一步：下载二进制包

[下载](https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz) 0.10.0.0版本并解压。

    > tar -xzf kafka_2.11-0.10.0.0.tgz
    > cd kafka_2.11-0.10.0.0

## 第二步：启动服务

kafka使用ZooKeeper，所以你需要先启动一个ZooKeeper服务。你可以方便地使用打包在kafka中的脚本来启动一个快餐式的单节点ZooKeeper实例。

    > bin/zookeeper-server-start.sh config/zookeeper.properties
    [2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties(org.apache.zookeeper.server.quorum.QuorumPeerConfig)
    ...

现在启动Kafka服务：

    > bin/kafka-server-start.sh config/server.properties
    [2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
    [2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
    ...

## 第三步：创建一个topic

让我们创建一个名为"test"的topic，有一个分区和一个副本：

    > bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test

我们现在可以通过运行下面的list topic命令看到这个新建的topic：

    > bin/kafka-topics.sh --list --zookeeper localhost:2181
    test

或者，除了手动创建topics之外，你还可以将你的brokers配置为在有消息发布到一个不存在的topic时自动创建topic。

## 第四步：发送一些消息

Kafka有一个命令行客户端可以将文件或标准输入作为输入，并将它作为消息流发送到Kafka集群。默认每一行作为一个单独的消息。

运行下面的producer，然后在终端输入一些消息并发送给服务端。

    > bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
    This is a message
    This is another message

## 第五步：启动一个consumer

Kafka有一个命令行consumer会将brokers中的消息打印到标准输出。

    > bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
    This is a message
    This is another message

如果上面的每个命令都运行在不同的终端，你现在就可以在producer终端输入一些消息，然后在consumer终端看到这些消息的输出。

所有的命令行工具都有附加的选项；不带选项运行这些命令会打印更多的实用信息。

## 第六步：创建一个多broker的Kafka集群

到目前为止，我们已经运行了一个单broker，但是这不够有趣。对于Kafka来说，单broker只是一个规模为1的集群，因此启动多个broker实例也还是一个集群。为了感受下多brokers，让我们扩展集群到3个节点（还是在一台主机上）。

首先我们为每个broker创建一个config文件：

    > cp config/server.properties config/server-1.properties
    > cp config/server.properties config/server-2.properties

现在编辑这些文件并设置如下的属性：

    config/server-1.properties:
        broker.id=1
        listeners=PLAINTEXT://:9093
        log.dir=/tmp/kafka-logs-1

    config/server-2.properties:
        broker.id=2
        listeners=PLAINTEXT://:9094
        log.dir=/tmp/kafka-logs-2

broker.id属性是集群中每个节点的唯一永久名称。我们覆盖服务端口和日志目录只是因为我们是在同一台主机上运行这些服务，避免多个broker都注册到同一个端口或覆盖彼此的数据目录。

我们已经有Zookeeper，并且有一个broker已经启动，因此我们只需要启动两个新的broker节点：

    > bin/kafka-server-start.sh config/server-1.properties &
    ...
    > bin/kafka-server-start.sh config/server-2.properties &
    ...

现在我们创建一个副本为3的新topic：

    > bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic

这样就可以了，但是现在有一个3个节点的集群，我们如何知道每个broker都在做什么？为了达到这个目的可以运行"describe topics"命令：

    > bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
    Topic:my-replicated-topic	PartitionCount:1	    ReplicationFactor:3	Configs:
        Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0

下面是对"describe topics"命令输出的解释。第一行输出给出了一个所有分区分区的汇总，其他每一行给出了单个分区的信息。因为当前这个topic只有一个分区，因此只有一行单个分区描述。

* "leader"是负责特定分区读写的（broker）节点。集群中的每个节点都会是分区中随机一部分的leader。
* "replicas"是一个复制特定分区日志的节点列表，对于"replicas"列表中的节点，不管它是否为leader以及它是否还存活。
* "isr"是一组"in sync" replicas。这是副本列表中当前存活并和leader同步的子集。

我们应该注意到，在上面的例子中节点1是topic唯一分区的leader。

我们可以在最开始的topic上运行同样的命令：

    > bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
    Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
        Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0

可以看到没有任何惊喜-最开始的topic没有副本并且在server 0上，是我们创建这个topic时集群中唯一的server。

让我们发布一些消息到新创建的topic：

    > bn/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
    ...
    my test message 1
    my test message 2
    ^C

现在我们可以消费这些消息：

    > bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
    ...
    my test message 1
    my test message 2
    ^C

现在让我们来测试容错。broker 1扮演leader的角色，所以我们kill它：

    > ps | grep server-1.properties
    7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...
    > kill -9 7564

leader角色已经转到其中一个slaves上，并且node 1 不再是in-sync副本集合（ISR）中的成员：

    > bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
    Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
        Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0

但是消息仍然可用于消费，但是最开始用于写入的leader（node 1）已经down掉了：
    
    > bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
    ...
    my test message 1
    my test message 2
    ^C

## 第七步：使用Kafka Connect导入导出数据

从console写入数据并将写入的数据写回console是一个不错的起点，但是你可能想要使用其他数据源的数据或者把Kafka中的数据导出到其他系统。对于很多系统来说，你可以使用Kafka Connect导入导出数据，从而代替编写特定的集成代码。Kafka Connect是一个扩展工具，它运行实现用于和外部系统交互的定制化逻辑的**_connector_**。在这篇快速入门中，，我们将看到如何使用Kafka Connect，用简单的connectors实现从一个文件导入数据到Kafka topic，然后将数据从Kafka topic导出到一个文件。首先，我们通过创建一些用于测试的种子数据开始：

    > echo -e "foo\nbar" > test.txt

然后，我们启动两个运行在standalone模式的connector，这以为这它们都运行在单一、本地、专用的进程中。我们提供三个配置文件作为参数。第一个配置文件用于Kafka Connect进程，包含诸如连接的Kafka brokers以及数据的序列化格式等通用配置。余下的配置文件每个对应一个要创建的connector。这些文件包含唯一的connector名称、要初始化的connector class、以及其他connector所需的配置。

    > bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties

这些包含在Kafka二进制文件包中简单的配置文件，使用默认的本地集群配置并创建两个connector：第一个是source connector用于从文件中读取行，然后将每一行发布到Kafka topic中；第二个是一个sink connector，从Kafka topic中读取消息，然后将每个消息作为一行生成到一个输出文件。在启动期间你回看到log消息的数量，包含一些connector被初始化的打印。一旦Kafka Connect进程被启动，source connector就应该开始从

    test.txt

文件中读取行，然后将这些行生产到topic

    connect-test

中，然后sink connector就应该开始从

    connect-test

这个topic中读取消息，然后将消息写到文件

    test.sink.txt

中。我们可以通过检查输出文件的内容验证数据已经通过整个管道传递成功：

    > cat test.sink.txt
    foo
    bar

需要注意的是，数据已经存储在Kafka topic

    connect-test

中，因此我们同样可以运行一个console consumer查看这个topic中的数据（或使用特定的consumer代码去处理它）：

    > bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic conect-test --from-beginning
    {“schema”:{"type":"string","optional":false},"payload":"foo"}
    {"schema":{"type":"string","optional":false},"payload":"bar"}

connector会一直处理数据，因此我们可以添加数据到输入文件中并查看数据在管道中移动的情况：
    
    > echo "Another line" >> test.txt

你应该可以看到输入的行出现在console consumer的输出以及sink文件中。

## 第八步：使用Kafka Streams处理数据

Kafka Streams是一个Kafka的客户端代码库，用于实时流处理以及分析存储在Kafka brokers中的数据。下面的快速入门示例会展示如何使用这个库运行一个流式应用程序代码。这里有一个WordCountDemo示例代码的要点（已经转换成Java 8的lambda表达式方便阅读）。

    KTable wordCounts = textLines;
    // Split each text line, by whitespace, into words.
    .flatMapValues(value -> Arrays.asList(value,toLowerCase().split("\\W+")))

    // Ensure the words are available as record keys for the next aggregate operation.
    .map((key, value) -> new KeyValue<>(value, value))

    // Count the occurrences of each word (record key) and store the results into a table named "Counts".
    .countByKey("Counts")

这段代码实现了WordCount算法，计算输入文本中单词出现的次数。不同于之前你可能看过的在有界数据集上运行的WordCount例子，这个WordCount示例程序表现有些不同，因为它被设计用于无穷无界的数据流。和有界变种类似的是，它是一个有状态的算法，记录和更新单词计数。然而，由于它必须假设潜在的无界输入数据，因此会在继续处理更多数据的过程中周期性地输出它当前的状态和结果，这是因为它不会知道何时才是处理了"所有的"输入数据。

我们现在准备输入数据到一个Kafka topic，这个数据集稍后会被一个Kafka Streams程序处理。

    > echo -e "all streams lead to kafka\nhello kafka streams\njoin kafka summit" > file-input.txt

然后，我们使用console producer将这个输入数据发送到名为streams-file-input的topic（实际应用场景中，流数据会持续不断地发送到Kafka，程序会一直保持运行）：

    > bin/kafka=-topics.sh --create \
                --zookeeper localhost:2181 \
                --replication-factor 1 \
                --partitions 1 \
                --topic streams-file-input

    > cat file-input.txt | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input

我们现在可以运行WordCount示例程序处理输入数据：

    > bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo

这里不会有任何STDOUT输出，作为结果的日志条目会源源不断地写回到另一个名为**_streams-wordcount-output_**的topic中。这个示例程序会运行几秒然后，不同于标准的流处理程序，自动终止。

我们现在从示例程序输出的topic中读取数据来检查WordCount示例程序的输出：

    > bin/kafka-console-consumer.sh --zookeeper localhost:2181 \
                --topic streams-wordcount-output \
                --form-beginning \
                --formatter kafka.tools.DefaultMessageFormatter \
                --property print.key=true \
                --property print.value=true \
                --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
                --property value.deserializer=org.apache.kafka.common.serialization.LongDeserialiser

下面的输出数据会打印到终端：

    all        1
    streams    1
    lead       1
    to         1
    kafka      1
    hello      1
    kafka      2
    streams    2
    join       1
    kafka      3
    summit     1

这里，第一列是Kafka消息的key，第二列是消息的值，所有都是java.lang.String格式。需要注意的是，**输出实际上是一个连续更新的数据流，每条数据记录（也就是上面原始输出的每一行）是一个单词更新的计数，这里单词也就是记录的key，例如"kafka"。对于有同一个key的多条记录，每条后续的记录都是前一条记录的更新**。

现在你可以向streams-file-input topic写入更多消息并观察加入streams-wordcount-output topic的消息，加入的消息反映了更新的单词计数（例如，使用上面用到的console producer和console consumer）。

你可以通过Ctrl-C来停止console consumer。
