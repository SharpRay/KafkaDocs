# 应用场景

这篇文档介绍了一些Apache Kafka流行的应用场景。对于这些领域的一些具体实践，可以参考[这篇博文](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)。

## 消息系统

Kafka能够很好地替换更为传统的message broker。使用message brokers有很多原因（从数据producer中解耦、缓存尚未处理的消息等等）对比大多数消息系统，Kafka有更好的吞吐、内建分区、分区复制、以及容错。这些特性使得Kafka对于大规模消息处理程序来说是一个非常好的解决方案。

在我们的实践中，消息系统一般来说相对有较低的吞吐量，但是需要保证端到端的低延迟以及强大的持久化能力，而这些都是Kafka能够提供的。

## 网站行为跟踪

Kafka最开始的应用案例是能够用来重建用户行为跟踪管道，作为一组实时的发布订阅源。这意味着网站活动（页面浏览，搜索，或其他用户行为）会发布到集中的topics中，每个topic负责一种行为类型。这些数据源可以在包括实时处理、实时监控、加载进Hadoop或者用于离线处理和分析的数据仓库等场景中实现订阅。

行为跟踪一般会有非常大的数据量，因为每个用户浏览页面都会产生很多行为日志。

## 指标度量

Kafka也用于操作的监控数据。这涉及从分布式应用中聚合统计数据，产出集中的操作数据的数据源。

## 日志聚合

很多人使用Kafka作为日志聚合的解决方案。日志聚合场景一般会从服务器上收集日志文件，将这些文件放在一个集中的存储区域（可能是一个文件服务器或者HDFS）用于后续处理。Kafka抛开文件细节，给出一个纯粹的日志或事件数据抽象，即将日志或者事件作为一个消息流。这使得低延迟处理和多数据源支撑以及分布式数据消耗的场景的实现变得更加简单。对比其他的日志收集系统，如Scribe或Flume，Kafka提供了相对更高性能、更强的持久化保证，这得益于它的复制机制，和更低的端到端的延迟。

# 流处理

很多Kafka的用户在包含多个阶段的处理管道中处理数据，管道中的原始输入数据从Kafka的topics中被消耗，接着会被聚合、展开、或经过处理之后放入新的topics用于进一步处理。例如，一个用于推荐新文章的数据处理管道可能会从RSS源爬取文章内容，然后发布到一个"articles"主题；后续的处理可能会将数据标准或将内容排重，将处理过的文章发布到一个新的topic；最后的处理阶段可能会将内容推荐给终端用户。这样的处理管道基于topic创建了实时数据流图。从Kafka 0.10.00开始，用户可以使用一个轻量但是强大的流处理库，叫做[Kafka Streams](http://kafka.apache.org/documentation.html#streams_overview)，去完成上述的数据处理工作。除了Kafka Streams，可选的开源流处理工具包括[Apache Storm](https://storm.apache.org/)和[Apache Samza](http://samza.apache.org/)。

## Event Sourcing

[Event Sourcing](http://martinfowler.com/eaaDev/EventSourcing.html)是一个应用程序设计的模式，程序状态变化会被记录成一个按时间的纪录序列。Kafka支持大量的日志数据，使得它非常适合用于## 这种模式的应用。

## 提交日志（Commit Log）

Kafka作为一个分布式系统的外部提交日志。日志用于在节点之间复制数据，并用于失效节点恢复数据的再同步机制。Kafka中的[log compaction](http://kafka.apache.org/documentation.html#compaction)特性支持这种用法。在这种场景下，Kafka比较像[Apache BookKeeper](http://zookeeper.apache.org/bookkeeper/)项目。

