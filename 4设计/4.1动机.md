# 动机

我们设计Kafka能够作为一个统一的平台去处理一个大公司可能具备的实时数据源（的数据量）。为了达到这个目的我们必须要考虑相对广泛的应用场景。

它应该具有很高的吞吐以支持诸如实时日志聚合这样的海量事件数据流。

它应该能够优雅地处理大量的日志，能够支持从离线系统中完成周期性的数据加载。

它应该是低延迟的系统，能够胜任更多的传统消息应用场景。

我们想要支持分区、分布式、实时处理这些数据源，基于这些数据源可以创建新的，派生的数据源。这种想法驱动了我们的分区机制和consumer模型的实现。

最后，在数据流给到其他数据系统用于服务的场景中，我们知道要在硬件失效的情况下需要保证容错。

对这些应用场景的支持，驱动我们完成一个具有多个独立元素的设计，比起传统消息系统更像是一个数据库日志。在下面的章节中，我们会介绍Kafka设计中的一些元素。

