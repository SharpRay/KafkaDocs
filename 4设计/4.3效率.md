# 效率

我们在保持高效方面做了很大努力。我们的应用场景之一是处理网站活动数据，这些数据量非常大：每个页面浏览会产生很多写入。更进一步讲。我们假设每条发布的消息至少被一个consumer（通常有很多）读取，因此我们力争使消费行为变得足够便宜。

我们还发现，从构建和运行多个相似系统的经验来看，效率是多租户高效操作的关键。如果下游的基础服务很容易因为应用程序造成的使用上的小碰撞变成瓶颈，这样的小变化通常会产生问题。我们帮助保证在基础服务之前的应用程序有较低负载、这在尝试运行支持成百上千应用程序的集中式服务中非常重要，因为在这样的集群中，使用模式的变化几乎每天都会发生。

我们在前面一节讨论了磁盘效率问题。一旦消除了低效的磁盘访问模式，在这样的系统中会有两种低效的常见原因：太多小的I/O操作，以及过多的字节拷贝。

小I/O问题在客户端和服务器之间以及服务器自己的持久化操作中都会发生。

为了避免这个问题，我们的协议围绕一个"消息集合"抽象建立，天然地将消息集合到一起。这使得网络请求将日志聚合到一起，并且分摊网络发送接收请求的开销，而不是每次发送一条消息。服务端依次追加消息块到它的日志，consumer一次fetch连续的大数据块。

这个简单的优化能够大规模提升性能。batching机制导致更大的网络数据包，更大的顺序磁盘操作，哦按徐的内存块等等，所有的这些使得Kafka将一个突发的随机消息写入变成流到consumers的顺序写入。

另一个不高效的地方在字节拷贝。较少的message没有这个问题，但是高负载下的影响是很明显的。为了避免这种情况我们使用了一个producer，broker以及consumer共享的标准化的二进制消息格式（这样数据块就可以在它们之间没有任何修改的传输了）。

broker维护的消息日志本身只是目录下的多个文件，每个文件被消息集合的序列填充，以和producer以及consumer相同的格式写入到磁盘。维护这种通用的格式允许对最重要的操作-持久化日志数据块的网络传输-的优化。现代unix操作系统提供了一个高度优化的从cachepage到socket的传输数据的代码路径；在Linux中这是通过[sendfile system call](http://man7.org/linux/man-pages/man2/sendfile.2.html)完成的。

要理解sendfile的影响，需要理解从文件到socket的一般的数据传输路径：

1. 操作系统将数据从磁盘读到内核空间的pagecache
2. 应用程序将数据从内核空间读到一个用户空间缓冲区
3. 应用程序将数据写回到内和空间的一个socket缓冲区
4. 操作系统将数据从socket缓冲区拷贝到NIC的缓冲区，在哪里数据在网络上被发送

这显然不够高效，这个过程中有4次拷贝和2次系统调用。使用sendfile，通过允许OS从pagecache直接发送数据到网络从而避免了重复拷贝。因此在这个优化路径中，只有最后数据拷贝到NIC缓冲区的操作是必须的。

我们期望的一个常见应用场景是在一个topic上有多个consumer。使用上面的零拷贝优化，数据被拷贝到pagecache一次并且在每次消费行为中重用，从而代替存储在内存并在每次读取时从内核空间拷出。这使得消息以接近网络连接极限性能的速率被消费。

这种pagecache和sendfile的结合意味着在一个Kafka集群中你看不到在磁盘上的读操作，数据完全是从cache中读取的。

对于更多的sendfile和零拷贝如何在Java中支持的更多信息，请阅读[这篇文章](http://www.ibm.com/developerworks/linux/library/j-zerocopy)

## 端到端的批量压缩

在有些场景中，瓶颈不是CPU或磁盘而是网络带宽。这种情况在需要在数据中心之间通过广域网发送消息的数据管道场景中体现的特别明显。用户当然能够一次压缩一条消息，这并不需要Kafka特性提供支持。但是这回导致非常低的压缩比率，这是因为大多的数据冗余是由于重复而存在于相同类型的消息之间的（例如JSON的字段名称或者网站日志的user agents或者相同的字符串值）。高效压缩需要多条消息一起压缩，而不是单条日志各自压缩。

Kafka通过允许递归消息集合来支持这一点。一批消息集合到一起压缩并以这种形式发送到服务端。这批消息会以压缩的形式写入并在日志中保持这种压缩格式，只是在consumer端会被解压缩。

Kafka支持GZIP,Snappy和LZ4压缩协议。跟多关于压缩的信息在[这里](https://cwiki.apache.org/confluence/display/KAFKA/Compression)

